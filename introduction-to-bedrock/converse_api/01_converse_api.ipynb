{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work with Converse API in Amazon Bedrock - Getting Started.\n",
    "\n",
    "*Note: This notebook has been adapted from the [Getting started with the Converse API in Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/Getting_started_with_Converse_API.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
    "\n",
    "To use the Converse API, you call the `Converse` or `ConverseStream` operations to send messages to a model. To call Converse, you require permission for the `bedrock:InvokeModel` operation. To call ConverseStream, you require permission for the `bedrock:InvokeModelWithResponseStream` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access). \n",
    "    \n",
    "    We have used below Foundation Models in our examples in this Notebook in `us-west-2` (Oregon) region.\n",
    "    \n",
    "| Provider Name | Foundation Model Name | Model Id |\n",
    "| ------- | ------------- | ------------- |\n",
    "| Amazon | Titan Text G1 - Express  | amazon.titan-text-express-v1 |\n",
    "| Amazon | Titan Text G1 - Lite | amazon.titan-text-lite-v1 |\n",
    "| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n",
    "| Anthropic | Claude 3 Haiku  | anthropic.claude-3-haiku-20240307-v1:0 |\n",
    "| Cohere | Command R+ | cohere.command-r-plus-v1:0 |\n",
    "| Cohere | Command R | cohere.command-r-v1:0 |\n",
    "| Meta | Llama 3.1 70B Instruct | meta.llama3-1-70b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 8B Instruct | meta.llama3-1-8b-instruct-v1:0 |\n",
    "| Mistral AI | Mistral Large 2 (24.07) | mistral.mistral-large-2407-v1:0 |\n",
    "| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the cells in this section to install the packages needed by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Using cached boto3-1.35.10-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.10 (from boto3)\n",
      "  Using cached botocore-1.35.10-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3)\n",
      "  Using cached s3transfer-0.10.2-py3-none-any.whl.metadata (1.7 kB)\n",
      "Collecting python-dateutil<3.0.0,>=2.1 (from botocore<1.36.0,>=1.35.10->boto3)\n",
      "  Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.36.0,>=1.35.10->boto3)\n",
      "  Using cached urllib3-2.2.2-py3-none-any.whl.metadata (6.4 kB)\n",
      "Collecting six>=1.5 (from python-dateutil<3.0.0,>=2.1->botocore<1.36.0,>=1.35.10->boto3)\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl.metadata (1.8 kB)\n",
      "Using cached boto3-1.35.10-py3-none-any.whl (139 kB)\n",
      "Using cached botocore-1.35.10-py3-none-any.whl (12.5 MB)\n",
      "Using cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.10.2-py3-none-any.whl (82 kB)\n",
      "Using cached python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Using cached urllib3-2.2.2-py3-none-any.whl (121 kB)\n",
      "Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Installing collected packages: urllib3, six, jmespath, python-dateutil, botocore, s3transfer, boto3\n",
      "  Attempting uninstall: urllib3\n",
      "    Found existing installation: urllib3 2.2.2\n",
      "    Uninstalling urllib3-2.2.2:\n",
      "      Successfully uninstalled urllib3-2.2.2\n",
      "  Attempting uninstall: six\n",
      "    Found existing installation: six 1.16.0\n",
      "    Uninstalling six-1.16.0:\n",
      "      Successfully uninstalled six-1.16.0\n",
      "  Attempting uninstall: jmespath\n",
      "    Found existing installation: jmespath 1.0.1\n",
      "    Uninstalling jmespath-1.0.1:\n",
      "      Successfully uninstalled jmespath-1.0.1\n",
      "  Attempting uninstall: python-dateutil\n",
      "    Found existing installation: python-dateutil 2.9.0.post0\n",
      "    Uninstalling python-dateutil-2.9.0.post0:\n",
      "      Successfully uninstalled python-dateutil-2.9.0.post0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.35.10\n",
      "    Uninstalling botocore-1.35.10:\n",
      "      Successfully uninstalled botocore-1.35.10\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.10.2\n",
      "    Uninstalling s3transfer-0.10.2:\n",
      "      Successfully uninstalled s3transfer-0.10.2\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.35.10\n",
      "    Uninstalling boto3-1.35.10:\n",
      "      Successfully uninstalled boto3-1.35.10\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "awscli 1.32.84 requires botocore==1.34.84, but you have botocore 1.35.10 which is incompatible.\n",
      "sagemaker-datawrangler 0.4.3 requires sagemaker-data-insights==0.4.0, but you have sagemaker-data-insights 0.3.3 which is incompatible.\n",
      "sphinx 7.2.6 requires docutils<0.21,>=0.18.1, but you have docutils 0.16 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed boto3-1.35.10 botocore-1.35.10 jmespath-1.0.1 python-dateutil-2.9.0.post0 s3transfer-0.10.2 six-1.16.0 urllib3-2.2.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mRunning boto3 version: 1.35.10\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall boto3\n",
    "\n",
    "import boto3\n",
    "import sys\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the region and models to use. We can also setup our boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using region:  us-west-2\n"
     ]
    }
   ],
   "source": [
    "region = 'us-west-2'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )\n",
    "\n",
    "MODEL_IDS = [\n",
    "    \"amazon.titan-text-express-v1\",\n",
    "    \"amazon.titan-text-lite-v1\",\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"cohere.command-r-plus-v1:0\",\n",
    "    \"cohere.command-r-v1:0\",\n",
    "    \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    \"mistral.mistral-large-2407-v1:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to setup our Converse API action in Bedrock. Note that we use the same syntax for any model, including the messages-formatted prompts, and the inference parameters. Also note that we read the output in the same way independently of the model used.\n",
    "\n",
    "Optionally, we could define additional model specific request fields that are not common across all providers. For more information on this check the Bedrock Converse API documentation.\n",
    "\n",
    "### Converse for one-shot invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "            #additionalModelRequestFields={\n",
    "            #}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Model invocation error\"\n",
    "    try:\n",
    "        result = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Output parsing error\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our invocation.\n",
    "\n",
    "In this example, we run the same prompt across all the text models supported in Bedrock by the time of writing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "\n",
      "Model: amazon.titan-text-express-v1\n",
      "The capital of Italy is Rome. It is the fourth most populous city in the European Union.\n",
      "--- Latency: 1258ms - Input tokens:10 - Output tokens:24 ---\n",
      "\n",
      "Model: amazon.titan-text-lite-v1\n",
      "The capital of Italy is Rome. Rome is the capital of both the country of Italy and the city of Rome.\n",
      "--- Latency: 1154ms - Input tokens:10 - Output tokens:27 ---\n",
      "\n",
      "Model: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Rome, also known as Roma in Italian, is not only the capital city but also the largest city in Italy. It has a rich history spanning over 2,500 years and is often referred to as the \"Eternal City.\" Some key facts about Rome include:\n",
      "\n",
      "1. It's located in the central-western portion of the Italian peninsula.\n",
      "\n",
      "2. It serves as the seat of the Italian government and is home to many important national institutions.\n",
      "\n",
      "3. Vatican City, an independent city-state and the headquarters of the Roman Catholic Church, is located entirely within Rome.\n",
      "\n",
      "4. Rome is renowned for its historical landmarks, including the Colosseum, the Pantheon, and the Roman Forum.\n",
      "\n",
      "5. It has been a major center of culture, art, and politics throughout history, playing a crucial role in the development of Western civilization.\n",
      "\n",
      "Rome has been the capital of Italy since 1871, shortly after the country's unification in 1861.\n",
      "--- Latency: 3959ms - Input tokens:14 - Output tokens:213 ---\n",
      "\n",
      "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
      "The capital of Italy is Rome.\n",
      "--- Latency: 280ms - Input tokens:14 - Output tokens:10 ---\n",
      "\n",
      "Model: cohere.command-r-plus-v1:0\n",
      "The capital of Italy is Rome.\n",
      "--- Latency: 298ms - Input tokens:7 - Output tokens:7 ---\n",
      "\n",
      "Model: cohere.command-r-v1:0\n",
      "The capital of Italy is Rome. A city steeped in a rich historical past, Rome is renowned for its cultural heritage, stunning architecture, and of course, as the former center of the Roman Empire. \n",
      "\n",
      "The city of Rome has been the capital of the Italian Republic since the country's unification in 1871. Rome's political significance dates back even further, as the capital of the Roman Kingdom, and subsequently the Roman Republic and Empire, for almost a millennium.\n",
      "\n",
      "With over 2.7 million residents, Rome is also Italy's third most populous city, after Milan and Naples. The city stands as an iconic destination, attracting visitors from around the world to witness its architectural masterpieces, such as the Colosseum and the Vatican City.\n",
      "--- Latency: 1740ms - Input tokens:7 - Output tokens:151 ---\n",
      "\n",
      "Model: meta.llama3-1-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "The capital of Italy is Rome (Italian: Roma).\n",
      "--- Latency: 725ms - Input tokens:21 - Output tokens:13 ---\n",
      "\n",
      "Model: meta.llama3-1-8b-instruct-v1:0\n",
      "\n",
      "\n",
      "The capital of Italy is Rome.\n",
      "--- Latency: 243ms - Input tokens:21 - Output tokens:9 ---\n",
      "\n",
      "Model: mistral.mistral-large-2407-v1:0\n",
      "The capital of Italy is Rome (Roma in Italian). Rome is also the country's most populated city and is known for its rich history, including the ancient Roman Empire, the Vatican City, and iconic landmarks like the Colosseum and the Roman Forum. It has been the capital of Italy since the country's unification in 1871.\n",
      "--- Latency: 1853ms - Input tokens:10 - Output tokens:81 ---\n",
      "\n",
      "Model: mistral.mixtral-8x7b-instruct-v0:1\n",
      " The capital of Italy is Rome. Known for its rich history, Rome is the largest city in Italy and serves as the country's political and cultural hub. It is home to a multitude of historical sites, including the Colosseum, the Roman Forum, and the Vatican City, which is an independent city-state enclaved within Rome. Rome has been the capital of Italy since 1871, following the country's unification.\n",
      "--- Latency: 1346ms - Input tokens:16 - Output tokens:98 ---\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
    "    print(f'Model: {i}\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConverseStream for streaming invocations\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: What is the capital of Italy?\n",
      "\n",
      "\n",
      "\n",
      "Model: amazon.titan-text-express-v1\n",
      "The capital of Italy is Rome. It is the fourth most populous city in the European Union.\n",
      "\n",
      "Model: amazon.titan-text-lite-v1\n",
      "The capital of Italy is Rome. Rome is the capital of both the country of Italy and the city of Rome.\n",
      "\n",
      "Model: anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Rome, also known as Roma in Italian, is not only the capital city but also the largest city in Italy. It has a rich history spanning over 2,500 years and is often referred to as the \"Eternal City.\" Some key facts about Rome include:\n",
      "\n",
      "1. It's located in the central-western part of the Italian peninsula.\n",
      "\n",
      "2. It serves as the seat of the Italian government and is home to many important political and cultural institutions.\n",
      "\n",
      "3. Vatican City, an independent city-state and the headquarters of the Roman Catholic Church, is located entirely within Rome.\n",
      "\n",
      "4. Rome is renowned for its ancient ruins, art, architecture, and historical significance, including landmarks such as the Colosseum, Roman Forum, and Pantheon.\n",
      "\n",
      "5. It has been the capital of Italy since 1871, after the country's unification.\n",
      "\n",
      "Rome is a major tourist destination and plays a significant role in Italy's economy, culture, and politics.\n",
      "\n",
      "Model: anthropic.claude-3-haiku-20240307-v1:0\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Model: cohere.command-r-plus-v1:0\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Model: cohere.command-r-v1:0\n",
      "The capital of Italy is Rome. A city steeped in a rich historical past, Rome is renowned for its cultural heritage, stunning architecture, and of course, as the former center of the Roman Empire. \n",
      "\n",
      "The city of Rome has been the capital of the Italian Republic since the country's unification in 1871. Rome's political significance dates back even further, as the capital of the Roman Kingdom, and subsequently the Roman Republic and Empire, for almost a millennium.\n",
      "\n",
      "With over 2.7 million residents, Rome is also Italy's third most populous city, after Milan and Naples. The city stands as an iconic destination, attracting visitors from around the world to witness its architectural marvels like the Colosseum, the Pantheon, and the Vatican City's St. Peter's Basilica.\n",
      "\n",
      "If you ever get the chance to visit Italy, Rome is a city that undoubtedly merits a place on your itinerary, offering an unforgettable blend of ancient history and modern-day charm.\n",
      "\n",
      "Model: meta.llama3-1-70b-instruct-v1:0\n",
      "\n",
      "\n",
      "The capital of Italy is Rome (Italian: Roma).\n",
      "\n",
      "Model: meta.llama3-1-8b-instruct-v1:0\n",
      "\n",
      "\n",
      "The capital of Italy is Rome.\n",
      "\n",
      "Model: mistral.mistral-large-2407-v1:0\n",
      "The capital of Italy is Rome (Roma in Italian). Rome is also the country's most populated city and is known for its rich history, including the ancient Roman Empire, the Vatican City, and iconic landmarks like the Colosseum and the Roman Forum. It has been the capital of Italy since the country's unification in 1871.\n",
      "\n",
      "Model: mistral.mixtral-8x7b-instruct-v0:1\n",
      " The capital of Italy is Rome. Known for its rich history, Rome is home to the Colosseum, the Roman Forum, and the Vatican City, which is the smallest internationally recognized independent state in the world, and an independent city-state enclaved within Rome. It is also the headquarters of the Catholic Church and where the Pope resides. Rome is a major European political, cultural, and financial center, and it is the third most-visited city in the European Union. It has a population of around 2.9 million people, and it is the most populated city in Italy."
     ]
    }
   ],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    print(f'\\n\\nModel: {i}')\n",
    "    invoke_bedrock_model_stream(bedrock, i, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation with model specific parameters using Converse API\n",
    "\n",
    "In this example we will call the Converse operation with the Anthropic Claude 3.5 Sonnet model. We will send the input text, inference parameters, and additional parameters that are unique to the model. We will start a conversation by asking the model to create a list of songs, then continues the conversation by asking that the songs are by artists from the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Generating message with model {model_id}')\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Input tokens: 45\n",
      "Output tokens: 57\n",
      "Total tokens: 102\n",
      "Stop reason: end_turn\n",
      "Generating message with model anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Input tokens: 117\n",
      "Output tokens: 69\n",
      "Total tokens: 186\n",
      "Stop reason: end_turn\n",
      "Role: user\n",
      "Text: Create a list of 3 pop songs.\n",
      "\n",
      "Role: assistant\n",
      "Text: Here's a list of 3 pop songs:\n",
      "\n",
      "1. \"Shape of You\" - Ed Sheeran\n",
      "2. \"Bad Guy\" - Billie Eilish\n",
      "3. \"Blinding Lights\" - The Weeknd\n",
      "\n",
      "Role: user\n",
      "Text: Make sure the songs are by artists from the United Kingdom.\n",
      "\n",
      "Role: assistant\n",
      "Text: I apologize for the oversight. Here's a list of 3 pop songs by artists from the United Kingdom:\n",
      "\n",
      "1. \"Waterloo\" - ABBA\n",
      "2. \"Someone You Loved\" - Lewis Capaldi\n",
      "3. \"Don't Start Now\" - Dua Lipa\n",
      "\n",
      "Finished generating text with model anthropic.claude-3-5-sonnet-20240620-v1:0.\n"
     ]
    }
   ],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
    "                    \"Only return song names and the artist.\"}]\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "}\n",
    "message_2 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "}\n",
    "messages = []\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    # Start the conversation with the 1st message.\n",
    "    messages.append(message_1)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    # Add the response message to the conversation.\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Continue the conversation with the 2nd message.\n",
    "    messages.append(message_2)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Show the complete conversation.\n",
    "    for message in messages:\n",
    "        print(f\"Role: {message['role']}\")\n",
    "        for content in message['content']:\n",
    "            print(f\"Text: {content['text']}\")\n",
    "        print()\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation with Image using Converse API\n",
    "\n",
    "\n",
    "In this example we will send an image as part of a message and requests that the model describe the image. The example uses Converse operation and the Anthropic Claude 3.5 Sonnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          input_text,\n",
    "                          input_image):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_image : The input image.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "\n",
    "    with open(input_image, \"rb\") as f:\n",
    "        image = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                    \"image\": {\n",
    "                        \"format\": 'jpeg',\n",
    "                        \"source\": {\n",
    "                            \"bytes\": image\n",
    "                        }\n",
    "                    }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model anthropic.claude-3-5-sonnet-20240620-v1:0\n",
      "Role: assistant\n",
      "Text: This image shows a group of nine dogs sitting together in a grassy field. The dogs are of various breeds and colors, including what appear to be Border Collies, a Belgian Malinois, and some mixed breeds. They're arranged in a line, facing the camera, with a beautiful green meadow and colorful flowers in the background. \n",
      "\n",
      "Some of the dogs have their tongues out, giving them a happy, friendly appearance. The dogs range in color from black, brown, and white, to combinations of these colors. Their ears are perked up, and they all seem alert and well-behaved, sitting still for the photo.\n",
      "\n",
      "The setting looks like a lovely summer day, with the field in full bloom. It's a charming and heartwarming image that showcases the diversity and beauty of different dog breeds.\n",
      "Input tokens:  1048\n",
      "Output tokens:  177\n",
      "Total tokens:  1225\n",
      "Stop reason: end_turn\n",
      "Finished generating text with model anthropic.claude-3-5-sonnet-20240620-v1:0.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "input_text = \"What's in this image?\"\n",
    "input_image = \"assets/sample_image.jpg\"\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = image_conversation(\n",
    "        bedrock_client, model_id, input_text, input_image)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    logger.error(\"A client error occurred: %s\", message)\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversation with Document using Converse API\n",
    "\n",
    "\n",
    "In this example, we will send a document as part of a message and requests that the model describe the contents of the document. The example uses Converse operation and the Meta Llama 3.1 8B Instruct Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def document_conversation(bedrock_client,\n",
    "                     model_id,\n",
    "                     input_text,\n",
    "                     input_document):\n",
    "    \"\"\"\n",
    "    Sends a message to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        input text : The input message.\n",
    "        input_document : The input document.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Generating message with model {model_id}\")\n",
    "\n",
    "    # Message to send.\n",
    "    \n",
    "    with open(input_document, \"rb\") as f:\n",
    "        doc_bytes = f.read()\n",
    "\n",
    "    message = {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\n",
    "                \"text\": input_text\n",
    "            },\n",
    "            {\n",
    "                \"document\": {\n",
    "                    \"name\": \"MyDocument\",\n",
    "                    \"format\": \"pdf\",\n",
    "                    \"source\": {\n",
    "                        \"bytes\": doc_bytes\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    messages = [message]\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages\n",
    "    )\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating message with model meta.llama3-1-8b-instruct-v1:0\n",
      "Role: assistant\n",
      "Text: \n",
      "\n",
      "This document is a letter to Amazon shareholders from the company's CEO, Andy Jassy, dated 2023. The letter discusses the company's performance, strategy, and future prospects. Here are some of the key points mentioned in the letter:\n",
      "\n",
      "1. **Amazon's performance**: The company's consumer business generated $434 billion in revenue in 2022, while its AWS business generated $80 billion in revenue.\n",
      "2. **Market opportunity**: The letter highlights the vast market opportunity in e-commerce and cloud computing, with 80% of global retail still residing in physical stores and 90% of global IT spending still on-premises.\n",
      "3. **Strategy**: Amazon's strategy is to focus on customer obsession, long-term market leadership, and relentless innovation.\n",
      "4. **Investments**: The company is investing heavily in areas such as artificial intelligence, machine learning, and cloud computing.\n",
      "5. **Future prospects**: The letter expresses optimism about Amazon's future prospects, citing the company's strong brand, customer loyalty, and ability to innovate and adapt to changing market conditions.\n",
      "6. **Reprint of the 1997 Shareholder Letter**: The letter includes a reprint of the 1997 Shareholder Letter from Jeff Bezos, which provides a historical context and a glimpse into the company's early days.\n",
      "\n",
      "Overall, the letter provides a comprehensive overview of Amazon's performance, strategy, and future prospects, and is likely to be of interest to investors, analysts, and anyone interested in the company's operations and direction.\n",
      "Input tokens:  8779\n",
      "Output tokens:  306\n",
      "Total tokens:  9085\n",
      "Stop reason: end_turn\n",
      "Finished generating text with model meta.llama3-1-8b-instruct-v1:0.\n"
     ]
    }
   ],
   "source": [
    "model_id = \"meta.llama3-1-8b-instruct-v1:0\" \n",
    "input_text = \"What's in this document?\"\n",
    "input_document = 'assets/2022-Shareholder-Letter.pdf'\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "\n",
    "    response = document_conversation(\n",
    "        bedrock_client, model_id, input_text, input_document)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "\n",
    "    print(f\"Role: {output_message['role']}\")\n",
    "\n",
    "    for content in output_message['content']:\n",
    "        print(f\"Text: {content['text']}\")\n",
    "\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens:  {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens:  {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens:  {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that we have seen the Converse API allow us to easily run the invocations with the same syntax across all the models, you can learn\n",
    "\n",
    "\n",
    "- How to do [function calling with the Converse API](../../agents/function-calling/function_calling_with_converse/function_calling_with_converse.ipynb)\n",
    "- How to work with [Converse API and Guardrails for Amazon Bedrock](../../responsible_ai/) **Link To be updated**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-west-2:236514542706:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
