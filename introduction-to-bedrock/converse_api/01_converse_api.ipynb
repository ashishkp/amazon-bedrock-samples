{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to work with Converse API in Amazon Bedrock - Getting Started.\n",
    "\n",
    "*Note: This notebook has been adapted from the [Getting started with the Converse API in Amazon Bedrock](https://github.com/aws-samples/amazon-bedrock-samples/blob/main/introduction-to-bedrock/Getting_started_with_Converse_API.ipynb)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "In this notebook, we'll explore the basics of the Converse API in Amazon Bedrock. The Converse or ConverseStream API is a unified structured text API action that allows you simplifying the invocations to Bedrock LLMs, using a universal syntax and message structured prompts for any of the supported model providers.\n",
    "\n",
    "To use the Converse API, you call the `Converse` or `ConverseStream` operations to send messages to a model. To call Converse, you require permission for the `bedrock:InvokeModel` operation. To call ConverseStream, you require permission for the `bedrock:InvokeModelWithResponseStream` operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prerequisites\n",
    "\n",
    "Before you can use Amazon Bedrock, you must carry out the following steps:\n",
    "\n",
    "- Sign up for an AWS account (if you don't already have one) and IAM Role with the necessary permissions for Amazon Bedrock, see [AWS Account and IAM Role](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#new-to-aws).\n",
    "- Request access to the foundation models (FM) that you want to use, see [Request access to FMs](https://docs.aws.amazon.com/bedrock/latest/userguide/getting-started.html#getting-started-model-access). \n",
    "    \n",
    "    We have used below Foundation Models in our examples in this Notebook in `us-west-2` (Oregon) region.\n",
    "    \n",
    "| Provider Name | Foundation Model Name | Model Id |\n",
    "| ------- | ------------- | ------------- |\n",
    "| Amazon | Titan Text G1 - Express  | amazon.titan-text-express-v1 |\n",
    "| Amazon | Titan Text G1 - Lite | amazon.titan-text-lite-v1 |\n",
    "| Anthropic | Claude 3.5 Sonnet  | anthropic.claude-3-5-sonnet-20240620-v1:0 |\n",
    "| Anthropic | Claude 3 Haiku  | anthropic.claude-3-haiku-20240307-v1:0 |\n",
    "| Cohere | Command R+ | cohere.command-r-plus-v1:0 |\n",
    "| Cohere | Command R | cohere.command-r-v1:0 |\n",
    "| Meta | Llama 3.1 405B Instruct | meta.llama3-1-405b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 70B Instruct | meta.llama3-1-70b-instruct-v1:0 |\n",
    "| Meta | Llama 3.1 8B Instruct | meta.llama3-1-8b-instruct-v1:0 |\n",
    "| Mistral AI | Mistral Large 2 (24.07) | mistral.mistral-large-2407-v1:0 |\n",
    "| Mistral AI | Mistral Large | mistral.mistral-large-2402-v1:0 |\n",
    "| Mistral AI | Mixtral 8X7B Instruct | mistral.mixtral-8x7b-instruct-v0:1 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Run the cells in this section to install the packages needed by this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -qU boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sys\n",
    "print('Running boto3 version:', boto3.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the region and models to use. We can also setup our boto3 client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = 'us-west-2'\n",
    "print('Using region: ', region)\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name = 'bedrock-runtime',\n",
    "    region_name = region,\n",
    "    )\n",
    "\n",
    "MODEL_IDS = [\n",
    "    \"amazon.titan-text-express-v1\",\n",
    "    \"amazon.titan-text-lite-v1\",\n",
    "    \"anthropic.claude-3-5-sonnet-20240620-v1:0\",\n",
    "    \"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    \"cohere.command-r-plus-v1:0\",\n",
    "    \"cohere.command-r-v1:0\",\n",
    "    \"meta.llama3-1-405b-instruct-v1:0\",\n",
    "    \"meta.llama3-1-70b-instruct-v1:0\",\n",
    "    \"meta.llama3-1-8b-instruct-v1:0\",\n",
    "    \"mistral.mistral-large-2407-v1:0\",\n",
    "    \"mistral.mistral-large-2402-v1:0\",\n",
    "    \"mistral.mixtral-8x7b-instruct-v0:1\"\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to setup our Converse API action in Bedrock. Note that we use the same syntax for any model, including the messages-formatted prompts, and the inference parameters. Also note that we read the output in the same way independently of the model used.\n",
    "\n",
    "Optionally, we could define additional model specific request fields that are not common across all providers. For more information on this check the Bedrock Converse API documentation.\n",
    "\n",
    "### Converse for one-shot invocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    try:\n",
    "        response = client.converse(\n",
    "            modelId=id,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"text\": prompt\n",
    "                        }\n",
    "                    ]\n",
    "                }\n",
    "            ],\n",
    "            inferenceConfig={\n",
    "                \"temperature\": temperature,\n",
    "                \"maxTokens\": max_tokens,\n",
    "                \"topP\": top_p\n",
    "            }\n",
    "            #additionalModelRequestFields={\n",
    "            #}\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Model invocation error\"\n",
    "    try:\n",
    "        result = response['output']['message']['content'][0]['text'] \\\n",
    "        + '\\n--- Latency: ' + str(response['metrics']['latencyMs']) \\\n",
    "        + 'ms - Input tokens:' + str(response['usage']['inputTokens']) \\\n",
    "        + ' - Output tokens:' + str(response['usage']['outputTokens']) + ' ---\\n'\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        result = \"Output parsing error\"\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can test our invocation.\n",
    "\n",
    "In this example, we run the same prompt across all the text models supported in Bedrock by the time of writing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    response = invoke_bedrock_model(bedrock, i, prompt)\n",
    "    print(f'Model: {i}\\n{response}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConverseStream for streaming invocations\n",
    "\n",
    "We can also use the Converse API for streaming invocations. In this case we rely on the ConverseStream action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock_model_stream(client, id, prompt, max_tokens=2000, temperature=0, top_p=0.9):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=id,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": prompt\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens,\n",
    "            \"topP\": top_p\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            sys.stdout.write(chunk['delta']['text'])\n",
    "            sys.stdout.flush()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\"What is the capital of Italy?\")\n",
    "print(f'Prompt: {prompt}\\n')\n",
    "\n",
    "for i in MODEL_IDS:\n",
    "    print(f'\\n\\nModel: {i}')\n",
    "    invoke_bedrock_model_stream(bedrock, i, prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converse API with Conversation and Model specific parameters\n",
    "\n",
    "In this example we will call the Converse operation with the Anthropic Claude 3.5 Sonnet model. We will send the input text, inference parameters, and additional parameters that are unique to the model. We will start a conversation by asking the model to create a list of songs, then continues the conversation by asking that the songs are by artists from the United Kingdom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_conversation(bedrock_client,\n",
    "                          model_id,\n",
    "                          system_prompts,\n",
    "                          messages):\n",
    "    \"\"\"\n",
    "    Sends messages to a model.\n",
    "    Args:\n",
    "        bedrock_client: The Boto3 Bedrock runtime client.\n",
    "        model_id (str): The model ID to use.\n",
    "        system_prompts (JSON) : The system prompts for the model to use.\n",
    "        messages (JSON) : The messages to send to the model.\n",
    "\n",
    "    Returns:\n",
    "        response (JSON): The conversation that the model generated.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    print(f'Generating message with model {model_id}')\n",
    "\n",
    "    # Inference parameters to use.\n",
    "    temperature = 0.5\n",
    "    top_k = 200\n",
    "\n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": temperature}\n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": top_k}\n",
    "\n",
    "    # Send the message.\n",
    "    response = bedrock_client.converse(\n",
    "        modelId=model_id,\n",
    "        messages=messages,\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "\n",
    "    # Log token usage.\n",
    "    token_usage = response['usage']\n",
    "    print(f\"Input tokens: {token_usage['inputTokens']}\")\n",
    "    print(f\"Output tokens: {token_usage['outputTokens']}\")\n",
    "    print(f\"Total tokens: {token_usage['totalTokens']}\")\n",
    "    print(f\"Stop reason: {response['stopReason']}\")\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botocore.exceptions import ClientError\n",
    "\n",
    "model_id = \"anthropic.claude-3-5-sonnet-20240620-v1:0\"\n",
    "\n",
    "# Setup the system prompts and messages to send to the model.\n",
    "system_prompts = [{\"text\": \"You are an app that creates playlists for a radio station that plays rock and pop music.\"\n",
    "                    \"Only return song names and the artist.\"}]\n",
    "message_1 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Create a list of 3 pop songs.\"}]\n",
    "}\n",
    "message_2 = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [{\"text\": \"Make sure the songs are by artists from the United Kingdom.\"}]\n",
    "}\n",
    "messages = []\n",
    "\n",
    "try:\n",
    "\n",
    "    bedrock_client = boto3.client(service_name='bedrock-runtime')\n",
    "\n",
    "    # Start the conversation with the 1st message.\n",
    "    messages.append(message_1)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    # Add the response message to the conversation.\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Continue the conversation with the 2nd message.\n",
    "    messages.append(message_2)\n",
    "    response = generate_conversation(\n",
    "        bedrock_client, model_id, system_prompts, messages)\n",
    "\n",
    "    output_message = response['output']['message']\n",
    "    messages.append(output_message)\n",
    "\n",
    "    # Show the complete conversation.\n",
    "    for message in messages:\n",
    "        print(f\"Role: {message['role']}\")\n",
    "        for content in message['content']:\n",
    "            print(f\"Text: {content['text']}\")\n",
    "        print()\n",
    "\n",
    "except ClientError as err:\n",
    "    message = err.response['Error']['Message']\n",
    "    print(f\"A client error occured: {message}\")\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        f\"Finished generating text with model {model_id}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "\n",
    "Now that we have seen the Converse API allow us to easily run the invocations with the same syntax across all the models, you can learn\n",
    "\n",
    "\n",
    "- How to do [function calling with the Converse API](../../agents/function-calling/function_calling_with_converse/function_calling_with_converse.ipynb)\n",
    "- How to work with [Converse API and Guardrails for Amazon Bedrock](../../responsible_ai/) **Link To be updated**\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bedrock-with-unified-api",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
